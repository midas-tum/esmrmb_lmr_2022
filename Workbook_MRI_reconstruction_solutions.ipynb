{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/midas-tum/esmrmb_lmr_2022/blob/main/Workbook_MRI_reconstruction_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FQfPO7E4wTw"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this exercise, you will get introduced to the world of MRI reconstruction. We start with examining the raw k-space data and coil-sensitivity maps and build the multi-coil forward and adjoint operator. Additionally, we solve a linear and a regularized reconstruction problem, which allows us to deeply understand where we can later connect to machine learning.\n",
        "\n",
        "First, we install the dependencies and download the data.\n",
        "\n",
        "Data were acquired on a 3T Siemens Magnetom Vida at the Institute of Biomedical Imaging, Graz University of Technology, Austria. Data should be only used for educational purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMZis3rdoWvf"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install PyWavelets git+https://github.com/khammernik/medutils.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbO9aJ7m4kq7"
      },
      "outputs": [],
      "source": [
        "# download data\n",
        "!wget -O brain_cartesian_2D.h5 https://www.dropbox.com/s/hclfv3re91qb1v3/brain_cartesian_2D.h5?dl=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ezeGwAE3ePu"
      },
      "outputs": [],
      "source": [
        "# Download ESPIRiT code for coil sensitivity map estimation\n",
        "!git clone https://github.com/mikgroup/espirit-python.git\n",
        "!cp /content/espirit-python/espirit.py ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN6i2t_WSAjw"
      },
      "source": [
        "# Magnetic Resonance Image (MRI) Reconstruction\n",
        "\n",
        "The goal is to recover the clean image $x$, which is obtained by undersampled k-space data $y$ and corrupted by additive Gaussian white noise $n$,\n",
        "$$ y = Ax + n. $$\n",
        "The rawdata $y$ was aquired for multiple receive coils. The linear operator $A$ denotes the mapping from image space to k-space.\n",
        "\n",
        "![MRI Inverse Problem](https://github.com/midas-tum/esmrmb_lmr_2022/blob/main/images/mri_inverse_problem.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIA9scmCZAsd"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "In the first step, we examine the avaiable data regarding their shape and their datatype. Note, that we are dealing with complex-valued data here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNj1r9CqS8Mh"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import medutils\n",
        "np.random.seed(1001)\n",
        "\n",
        "ds = h5py.File('./brain_cartesian_2D.h5', 'r')\n",
        "kspace = ds['kspace'][()]\n",
        "ds.close()\n",
        "\n",
        "print(f'K-Space:')\n",
        "print(f'dtype={kspace.dtype}')\n",
        "print(f'(nCoils, nFE, nPE)={kspace.shape}')\n",
        "nCoils, nFE, nPE = kspace.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_UitpW8VRdG"
      },
      "source": [
        "We observe that we have 16 coils, the number of frequency encoding (readout) points `nFE` equals 640, and the number of phase encoding steps `nPE` is 330. We will come back to this later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prqq7W7pUt9p"
      },
      "source": [
        "## Data Visualization\n",
        "For data visualization, you are free to use any plotting library such as `matplotlib` or use the provided `medutils` package. The `medutils` package has some useful function for visualization:\n",
        "- `kshow` Process the data in log-space\n",
        "- `imshow` Display the magnitude of the image\n",
        "- `plot_array` Re-arrange the images from a 3D array next to each other.\n",
        "- `ksave` Save k-space\n",
        "- `imsave` Save images\n",
        "\n",
        "We will first visualize the `kspace`. The data was acquired with 16 coils. The vertical direction is the frequency encoding (FE) direction, and the horizontal direction is the phase encoding (PE) direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO2e_OvXUwLV"
      },
      "outputs": [],
      "source": [
        "medutils.visualization.kshow(medutils.visualization.plot_array(kspace, M=2, N=8), title='K-space', figsize=(40,20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt6WTnNqXq9b"
      },
      "source": [
        "## Transforming k-space to image space\n",
        "Let us now start to transform the `kspace` to images. Therefore, we require the centered 2d inverse Fourier transform. Application of the `ifft2c` to the k-space results in individual coil images.\n",
        "\n",
        "**Task 1: Write the function `ifft2c(kspace)`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zD9fi0BVzvi"
      },
      "outputs": [],
      "source": [
        "def ifft2c(kspace):\n",
        "  #TODO implement the centered inverse FFT.\n",
        "  return np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(kspace), norm='ortho'))\n",
        "\n",
        "coil_img = ifft2c(kspace)\n",
        "medutils.visualization.imshow(medutils.visualization.plot_array(coil_img, M=2, N=8), title='Coil Images', figsize=(40,20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Em5MdDLkYBYy"
      },
      "source": [
        "You might notice several things. First, you see that only a fraction of the image is bright. This is due to the effect that the coils are sensitive only in a certain spatial region. Second, you might notice that there are a lot of black areas all over the image, especially in column direction. This extended field of view in read-out direction, also termed frequency-encoding direction is actually for free, i.e. does not cost any additional acquisition time, and is acquired per default on MRI scanners. Assuming the base resolution is 320, the number of frequency encoding steps is (at least) doubled. This frequency oversampling results in an increased field-of-view in this direction. After the image is transformed to image domain, only the central part needs to be visualized. Thus, for display, we will from now on only consider the central part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kbOdgVwUG1I"
      },
      "source": [
        "## Root-Sum-of-Squares Reconstruction\n",
        "\n",
        "Now, we calculate the Root-Sum-of-Squares reconstruction $x_{rss}$\n",
        "$$ x_{rss} = \\sqrt{ \\sum_{c=1}^{nCoils} \\vert x_c \\vert ^ 2 }, $$\n",
        "\n",
        "where $x_c$ are the individual coil images. Note that using the root-sum-of-squares reconstruction, the phase information of the complex-valued data gets lost. \n",
        "\n",
        "We now visualize only the central part of the reconstructed image of size `[nFE//2, nPE]`. \n",
        "\n",
        "**Task 2: Implement the RSS reconstruction `rss(coil_img)`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeYHaNA7VZXt"
      },
      "outputs": [],
      "source": [
        "def rss(coil_img):\n",
        "  #TODO implement the rss reconstruction\n",
        "  return np.sqrt(np.sum(np.abs(coil_img)**2, 0))\n",
        "\n",
        "x_rss = rss(coil_img)\n",
        "medutils.visualization.imshow(medutils.visualization.center_crop(x_rss, (nFE//2, nPE)), figsize=(10,10), title='RSS reconstruction')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izcaa68j5-Qa"
      },
      "source": [
        "# Sensitivity Map Estimation\n",
        "\n",
        "The coil sensitivity maps (`smaps`) are smooth maps that show us in which parts the individual coil elements are sensitive. We will need these information for our multi-coil MRI forward and adjoint operators. We use the [python implementation](https://github.com/mikgroup/espirit-python) for ESPIRiT [1,2] to estimate these coil sensitivity maps.\n",
        "\n",
        "[1] Uecker et al. [ESPIRiT—an eigenvalue approach to autocalibrating parallel MRI: Where SENSE meets GRAPPA](https://onlinelibrary.wiley.com/doi/10.1002/mrm.24751). Magn Reson Med 71(3):990-1001, 2014.\n",
        "\n",
        "[2] https://github.com/mikgroup/espirit-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP1mGhS559Zu"
      },
      "outputs": [],
      "source": [
        "import espirit\n",
        "kspace_espirit = np.transpose(kspace, (1, 2, 0))[:,:,np.newaxis]\n",
        "smaps_espirit = espirit.espirit(kspace_espirit, 8, 24, 0.05, 0)\n",
        "\n",
        "smaps = smaps_espirit[:,:,0,:,0]\n",
        "smaps = np.transpose(smaps, (2, 0, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQR3u1zulY9k"
      },
      "source": [
        "Let us visualize the coil sensitivity maps for our k-space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OS7HflObXC--"
      },
      "outputs": [],
      "source": [
        "medutils.visualization.imshow(medutils.visualization.plot_array(smaps), title='Sensitivity Maps  (compressed)', figsize=(40,20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSEJEA8GZEqK"
      },
      "source": [
        "# Multi-Coil Operators and Sensitivity-Weighted coil combination\n",
        "\n",
        "Now, we have all ingredients to combine the image! Do you remember how the forward and adjoint MRI multi-coil operators are defined? These are required in the next steps.\n",
        "\n",
        "**Task 4: Implement the multi-coil forward operator $A$ in `mriForwardOp(image, smaps, mask)` and adjoint operator $A^*$ in `mriAdjointOp(kspace, smaps, mask)`. Please check the lecture slides for details on the implementation.**\n",
        "\n",
        "*Hint: Start with the implementation of the adjoint operator and make use of the previously written function `ifft2c`. Then, define a function `fft2c` which is the 2D centered Fourier transform before you continue with the forward operator.*\n",
        "\n",
        "## Suggested Readings:\n",
        "\n",
        "Pruessmann et al. [SENSE: Sensitivity encoding for fast MRI](https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291522-2594%28199911%2942%3A5%3C952%3A%3AAID-MRM16%3E3.0.CO%3B2-S) Magnetic Resonance in Medicine, 43(5):952-962, 1999.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xxwj3aQBYpVi"
      },
      "outputs": [],
      "source": [
        "def mriAdjointOp(kspace, smaps, mask):\n",
        "  # TODO implement\n",
        "  return np.sum(ifft2c(kspace * mask)*np.conj(smaps), axis=0)\n",
        "\n",
        "def fft2c(image):\n",
        "  # TODO implement\n",
        "  return np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(image), norm='ortho'))\n",
        "\n",
        "def mriForwardOp(image, smaps, mask):\n",
        "  # TODO implement\n",
        "  return fft2c(smaps * image) * mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo1OdsSInbKG"
      },
      "source": [
        "Now, you should check if the adjoint operator is working as expected. The result should be a coil-combined image. Right now, there is no undersampling mask involved, i.e., it is set to all ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owqzxb74nVqO"
      },
      "outputs": [],
      "source": [
        "img_cc = mriAdjointOp(kspace, smaps, np.ones_like(kspace))\n",
        "medutils.visualization.imshow(medutils.visualization.center_crop(img_cc, (nFE//2, nPE)), title='Combined image (sensitivity-weighted)', figsize=(8,8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf3LX-qSn5SP"
      },
      "source": [
        "**Task 5: Adjointness check**\n",
        "\n",
        "Now, also check if the operators are adjoint using the following equation:\n",
        "$$ \\langle Au, v\\rangle = \\langle u, A^Hv\\rangle,$$\n",
        "where $u$ and $v$ are complex random numbers. The variable $u$ should have the same size as the image $x$ and $v$ should have the same size as the k-space $y$. Note, that the sampling mask and sensitivity maps are kept constant. To create random numbers, use `np.random.randn`. Print the results for the left-hand side and right-hand side of the equation.\n",
        "\n",
        "*Hint: To get the correct result, you might use the `conj` when computing the complex-valued dot product. Also, the forward and inverse Fourier transform have to be scaled the same way (`norm='ortho'`) to get correct results.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17WwxyIrn4gm"
      },
      "outputs": [],
      "source": [
        "# TODO implement the adjointness check\n",
        "u = np.random.randn(*img_cc.shape) + 1j * np.random.randn(*img_cc.shape)\n",
        "v = np.random.randn(*kspace.shape) + 1j * np.random.randn(*kspace.shape)\n",
        "\n",
        "\n",
        "lhs = np.sum(np.conj(mriForwardOp(u, smaps, np.ones_like(kspace))) * v)\n",
        "rhs = np.sum(np.conj(u) * mriAdjointOp(v, smaps, np.ones_like(kspace)))\n",
        "\n",
        "print(f'{lhs} == {rhs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Fv8DY7ZxaZ"
      },
      "source": [
        "# Undersampling\n",
        "Now we will get to the most exciting part of this exercise - undersampling the k-space! We will generate undersampling masks for acceleration $R\\in\\lbrace 2,3\\rbrace$. \n",
        "\n",
        "**Task 5: Your task is to play around with different undersampling patterns. Generate a sampling pattern in the function `generate_mask(R, nPE, nFE, mode)` where `R` is the acceleration factor and `mode` is an integer corresponding to following patterns:**\n",
        "1. Choose randomly an integer {0,1} with propability `p=[1-1/R, 1/R]`\n",
        "2. Only set a dense block of `nRef=20` lines in the center of k-space.\n",
        "3. Combine 1.+2.\n",
        "4. Only set every `R`-th line\n",
        "5. Combine 2.+4. \n",
        "\n",
        "To create the mask, simply generate a 1D line of size `nPE`. For each item, compute and print the effective acceleration `Reff`, which is determined by `nPE` divided by the number of sampled points. \n",
        "\n",
        "The code to get the full mask of size `[nFE, nPE]` is given below.\n",
        "\n",
        "To continue the tasks on iterative reconstruction, please use `mode=3` and `R=3` as well as `mode=5` and `R=3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08GrS1rcZw6W"
      },
      "outputs": [],
      "source": [
        "# TODO generate undersampling masks\n",
        "def generate_mask(R, nPE, nFE, mode):\n",
        "  nRef = 20\n",
        "  if mode == 1:\n",
        "    mask = np.random.choice([1, 0],(nPE),p=[1/R, 1-1/R])\n",
        "  elif mode == 2:\n",
        "    mask = np.zeros(nPE)\n",
        "    mask[nPE//2-nRef//2:nPE//2+nRef//2] = 1\n",
        "  elif mode == 3:\n",
        "    mask = np.random.choice([1, 0],(nPE),p=[1/R, 1-1/R])\n",
        "    mask[nPE//2-nRef//2:nPE//2+nRef//2] = 1\n",
        "  elif mode == 4:\n",
        "    mask = np.zeros(nPE)\n",
        "    mask[::R] = 1\n",
        "  elif mode == 5:\n",
        "    mask = np.zeros(nPE)\n",
        "    mask[::R] = 1\n",
        "    mask[nPE//2-nRef//2:nPE//2+nRef//2] = 1\n",
        "  else:\n",
        "    raise ValueError(f'Mode {mode} not defined')\n",
        "\n",
        "  Reff = nPE/np.sum(mask)\n",
        "  print(f'Reff={Reff}')\n",
        "\n",
        "  mask = mask.reshape(1, nPE).repeat(nFE, axis=0)\n",
        "\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIyzXD39zKQo"
      },
      "source": [
        "Now, generate the mask and visualize it (we visualize only a fraction in frequency encoding direction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nTeENqRzGig"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1001)\n",
        "mask = generate_mask(R=3, nPE=nPE, nFE=nFE, mode=5)\n",
        "\n",
        "medutils.visualization.imshow(mask[:40,:], 'Undersampling mask', figsize=(20,20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zy-F7VAcHYR"
      },
      "source": [
        "**Task 6: Zero-Filling solution**\n",
        "\n",
        "Now you are ready to estimate the zero filling solution by applying the adjoint operator to the data, by using the estimated undersampling mask `mask`. Play around with above mask configurations. How do the images change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFdaATXDb0E-"
      },
      "outputs": [],
      "source": [
        "#TODO Apply the adjoint operator to the data and use the newly created undersampling mask.\n",
        "img_cc_us = mriAdjointOp(kspace, smaps, mask)\n",
        "img_cc_us = medutils.visualization.center_crop(img_cc_us, (nFE//2, nPE))\n",
        "medutils.visualization.imshow(img_cc_us, 'Undersampled image (zero filling)', figsize=(10,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFKJhOEBchbF"
      },
      "source": [
        "# Linear and Regularized Reconstruction\n",
        "Now, we are ready to implement linear and regularized reconstruction. We additionally need the gradient operator, implementing forward / backward differences in `D` and `DT`, and the multi-coil MRI forward and adjoint operators, `A` and `AH`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM58ogWNc4M6"
      },
      "outputs": [],
      "source": [
        "def nabla(x):\n",
        "    dx = np.pad(x[:,1:], [[0, 0],[0, 1]], mode='edge')\n",
        "    dy = np.pad(x[1:], [[0, 1],[0, 0]], mode='edge')\n",
        "    return np.concatenate([dx[None,...] - x, dy[None,...] - x], 0)\n",
        "\n",
        "def nablaT(x):\n",
        "    assert x.shape[0] == 2\n",
        "    dx = np.pad(x[0,:,:-1], [[0, 0],[1, 1]], mode='constant')\n",
        "    dy = np.pad(x[1,:-1], [[1, 1],[0, 0]], mode='constant')\n",
        "    return dx[:,:-1] - dx[:,1:] + dy[:-1] - dy[1:]\n",
        "\n",
        "D = lambda x: nabla(np.real(x)) + 1j * nabla(np.imag(x))\n",
        "DT= lambda x: nablaT(np.real(x)) + 1j * nablaT(np.imag(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anJOsdUec7cz"
      },
      "outputs": [],
      "source": [
        "A = lambda x: mriForwardOp(x, smaps, mask)\n",
        "AH = lambda x: mriAdjointOp(x, smaps, mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiFZ3aR7dT_O"
      },
      "source": [
        "## Solving the linear reconstruction problem\n",
        "Consider the following minimization problem:\n",
        "\n",
        "$$ \\min_x  E(x,y) = \\min_x \\frac{1}{2} \\Vert Ax - y \\Vert_2^2 .$$\n",
        "\n",
        "While in image denoising we are still able to compute a closed-form solution for this problem, this is not feasible for the task of MRI reconstruction anymore. We instead use first-order optimization methods and solve this by Gradient Descent:\n",
        "$$ x^{k+1} = x^{k} - \\alpha \\nabla_x E(x,y) $$\n",
        "$$ x^{k+1} = x^{k} - \\alpha A^H (Ax^k - y) $$\n",
        "\n",
        "**Task 7: Implement Gradient Descent in `opt_linear` to solve the linear reconstruction problem and run the optimization for `max_iter=50` iterations and a step size of `alpha=1`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2h7OIo1dN64"
      },
      "outputs": [],
      "source": [
        "def opt_linear(y, max_iter, alpha):\n",
        "    x = np.zeros_like(AH(y))\n",
        "\n",
        "    #TODO implement gradient descent to solve the linear reconstruction problem\n",
        "    for _ in range(max_iter):\n",
        "        x = x - alpha * AH(A(x) - y)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTkILw7GeOEW"
      },
      "outputs": [],
      "source": [
        "alpha=1\n",
        "img_linear = opt_linear(kspace, max_iter=50, alpha=alpha)\n",
        "img_linear = medutils.visualization.center_crop(img_linear, (nFE//2, nPE))\n",
        "medutils.visualization.imshow(img_linear, f'Linear reconstruction alpha={alpha}', figsize=(10,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziFJr1Mpe0re"
      },
      "source": [
        "## L2-H1 Regularization\n",
        "\n",
        "Now, we regularize the least-squares problem with a regularizer of form $\\mathcal{R}(x)=\\frac{1}{2} \\Vert \\nabla x \\Vert_2^2$.\n",
        "Consider now the following minimization problem\n",
        "\n",
        "$$ \\min_x  D(x,y) + \\lambda R(x) = \\min_x \\frac{1}{2}\\Vert Ax - y \\Vert_2^2 + \\frac{\\lambda}{2}\\Vert \\nabla x \\Vert_2^2.$$\n",
        "\n",
        "We solve this by Gradient Descent:\n",
        "$$ x^{k+1} = x^{k} - \\alpha \\left( \\nabla_x D(x,y) + \\nabla_x R(x) \\right) $$\n",
        "$$ x^{k+1} = x^{k} - \\alpha \\left( A^H (Ax^k - y) + \\lambda \\nabla^T \\nabla x^k \\right) $$\n",
        "\n",
        "**Task 8: Implement gradient descent to solve the L2-H1 regularized problem and run the optimization for `max_iter=200` iterations, a step size of `alpha=1.0` and a regularization parameter of `lambd=0.01`.**\n",
        "\n",
        "*Note that we do not have the best setting for the parameters here and the difference to the linear reconstruction might be only minimal. You can play around with the hyper-parameters. This example is to show you the properties of L2-H1 regularization and that it is actually hard to find a good set of hyper-parameters (step size, regularization parameters, iterations).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIk8_dJJe0N8"
      },
      "outputs": [],
      "source": [
        "def opt_reg_l2(y, max_iter, alpha, lambd):\n",
        "    x = np.zeros_like(AH(y))\n",
        "    # TODO implement gradient descent for L2-H1 regularization\n",
        "    for _ in range(max_iter):\n",
        "        x = x - alpha * AH(A(x) - y) - alpha * lambd * DT(D(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rDK8F2Ie1gO"
      },
      "outputs": [],
      "source": [
        "alpha = 1.0\n",
        "lambd = 0.01\n",
        "img_reg_l2 = opt_reg_l2(kspace, max_iter=200, alpha=alpha, lambd=lambd)\n",
        "img_reg_l2 = medutils.visualization.center_crop(img_reg_l2, (nFE//2, nPE))\n",
        "medutils.visualization.imshow(img_reg_l2, f'L2H1 reconstruction alpha={alpha} lambda={lambd}', figsize=(10,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgHKKicfgExG"
      },
      "source": [
        "## Sparse MRI: Wavelet Thresholding\n",
        "Medical images per se are not sparse, however, they might have a sparse representation in some transform domain. One example here is the Wavelet transform, resulting in a multi-level feature representation. We provide the `plot_wavedec` function to find out how the sparse images look like at different scales and orientations.\n",
        "\n",
        "We perform an optimization first wrt. data consistency term. This is followed by a Wavelet transform, and the *detailed* Wavelet coefficients are surpressed by using soft-thresholding, i.e.,\n",
        "\n",
        "$$\n",
        "\\text{thresh}(x) = \\frac{x}{\\vert x \\vert}\\max(\\vert x \\vert - \\alpha\\lambda , 0)\n",
        "$$\n",
        "\n",
        "**Task 9: Implement the soft-thresholding in `soft_thresh(x, tau)`**\n",
        "\n",
        "*Hint: Note, that the absolut value could get zero, and a small epsilon might be adorable to surpress this.*\n",
        "\n",
        "### Suggested Readings\n",
        "\n",
        "Lustig et al. [Compressed Sensing MRI](https://ieeexplore.ieee.org/document/4472246), IEEE Signal Processing Magazine 25(2):72-82, 2008.\n",
        "\n",
        "Lustig et al. [Sparse MRI: The application of compressed sensing for rapid MR imaging](https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.21391). Magnetic Resonance in Medicine 58(6):1182-1195, 2007."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17LiCd6A_MOf"
      },
      "outputs": [],
      "source": [
        "def soft_thresh(x, tau):\n",
        "    #TODO: implement soft-thresholding\n",
        "    return x / np.maximum(np.abs(x), 1e-9) * np.maximum(np.abs(x) - tau, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIeuyuFtgEMm"
      },
      "outputs": [],
      "source": [
        "import pywt\n",
        "\n",
        "def plot_wavedec(img, wavelet='db4', level=2):\n",
        "    img = medutils.visualization.center_crop(img_cc, (nFE//2, nPE))\n",
        "    coeffs = pywt.wavedecn(img, wavelet=wavelet, level=level)\n",
        "    # normalize coeffs\n",
        "    coeffs[0] /= np.max(np.abs(coeffs[0]))\n",
        "    for level in range(1, len(coeffs)):\n",
        "        for key in coeffs[level].keys():\n",
        "            coeffs[level][key] /= np.max(np.abs(coeffs[level][key]))\n",
        "    arr, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
        "    medutils.visualization.imshow(arr, figsize=(10,10))\n",
        "\n",
        "def opt_reg_wavelet(y, max_iter, alpha, lambd, wavelet='db4', level=3):\n",
        "    x = np.zeros_like(AH(y))\n",
        "    wavelet_object = pywt.Wavelet(wavelet)\n",
        "    threshold = alpha * lambd\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        x = x - alpha * (AH(A(x) - y))\n",
        "        coeffs = pywt.wavedecn(x, wavelet_object, level=level)\n",
        "        array, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
        "        denoised_array=soft_thresh(array, threshold)\n",
        "        denoised_coeffs = pywt.array_to_coeffs(denoised_array, coeff_slices, output_format='wavedecn')\n",
        "        denoised_coeffs[0] = coeffs[0]\n",
        "        x = pywt.waverecn(denoised_coeffs, wavelet_object)\n",
        "        \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlHCMCmB_ECn"
      },
      "source": [
        "Next, we define a wavelet, the number of levels for decomposition and plot the decomposition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2RfjPmygfCw"
      },
      "outputs": [],
      "source": [
        "# Plot Wavelet Decomposition\n",
        "wavelet='bior2.8'\n",
        "level=3\n",
        "plot_wavedec(img_cc, wavelet, level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaB2pREX_Y0j"
      },
      "source": [
        "Finally, we run the optimization for `lambd=1e-6` and `alpha=1` and 200 iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnDICemEgub_"
      },
      "outputs": [],
      "source": [
        "lambd=1e-6\n",
        "alpha=1.0\n",
        "img_reg_wavelet = opt_reg_wavelet(kspace, max_iter=200, alpha=alpha, lambd=lambd, wavelet=wavelet, level=level)\n",
        "img_reg_wavelet = medutils.visualization.center_crop(img_reg_wavelet, (nFE//2, nPE))\n",
        "medutils.visualization.imshow(img_reg_wavelet, f'Wavelet reconstruction alpha={alpha} lambda={lambd}', figsize=(10,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connections to Machine Learning\n",
        "\n",
        "You might wonder, why there is not any exercise on machine learning? The tasks in this workbook form the cornerstone for successfull Machine Learning MRI reconstruction 😁. However, we would like to introduce you to the world of machine learning for MRI reconstruction in separate IPython notebooks 🤓:\n",
        "- [Image denoising (magnitude)](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_denoising_real.ipynb)\n",
        "- [Image denoising (2-channel real-valued)](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_denoising_2chreal.ipynb)\n",
        "- [Image denoising (complex-valued)](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_denoising_complex.ipynb)\n",
        "- [Image reconstruction (complex-valued)](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_reconstruction_complex.ipynb)\n",
        "- [Complex-valued activation functions](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_complex_activations.ipynb)\n",
        "- [Complex layers](https://github.com/midas-tum/merlin/blob/master/notebooks/tutorial_complex_layers.ipynb)\n"
      ],
      "metadata": {
        "id": "GIHKhUP9vwNc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3FQfPO7E4wTw"
      ],
      "name": "Workbook_MRI_reconstruction_solutions.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}